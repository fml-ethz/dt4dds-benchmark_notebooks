{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import dt4dds_benchmark\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_initcov = dt4dds_benchmark.analysis.Dataset.combine(*[dt4dds_benchmark.pipelines.HDF5Manager(f'./data/initcov/{s}.hdf5').get_data() for s in (\n",
    "    'aeon_high',\n",
    "    'aeon_low',\n",
    "    'aeon_medium',\n",
    "    'fountain_high',\n",
    "    'fountain_low',\n",
    "    'fountain_medium',\n",
    "    'goldman_default',\n",
    "    'rs_high',\n",
    "    'rs_low',\n",
    "    'rs_medium',\n",
    "    'hedges_low',\n",
    "    'hedges_medium',\n",
    "    'yinyang_default',\n",
    ")])\n",
    "data_seqdepth = dt4dds_benchmark.analysis.Dataset.combine(*[dt4dds_benchmark.pipelines.HDF5Manager(f'./data/seqdepth/{s}.hdf5').get_data() for s in (\n",
    "    'aeon_high',\n",
    "    'aeon_low',\n",
    "    'aeon_medium',\n",
    "    'fountain_high',\n",
    "    'fountain_low',\n",
    "    'fountain_medium',\n",
    "    'goldman_default',\n",
    "    'rs_high',\n",
    "    'rs_low',\n",
    "    'rs_medium',\n",
    "    'hedges_low',\n",
    "    'hedges_medium',\n",
    "    'yinyang_default',\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap_type = {'DNAAeon': '#3182bd', 'DNAFountain': '#31a354', 'DNARS': '#e6550d', 'HEDGES': '#756bb1', 'YinYang': '#636363', 'Goldman': '#de2d26'}\n",
    "\n",
    "colormap_name = {\n",
    "    'DNAAeon-High': '#08519c', 'DNAAeon-Medium': '#3182bd', 'DNAAeon-Low': '#6baed6',\n",
    "    'DNAFountain-High': '#006d2c', 'DNAFountain-Medium': '#31a354', 'DNAFountain-Low': '#74c476',\n",
    "    'DNARS-High': '#a63603', 'DNARS-Medium': '#e6550d', 'DNARS-Low': '#fd8d3c',\n",
    "    'HEDGES-Medium': '#756bb1', 'HEDGES-Low': '#9e9ac8',\n",
    "    'YinYang-High': '#636363', 'Goldman-Low': '#de2d26',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define functions to select pareto-optimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://stackoverflow.com/questions/32791911/fast-calculation-of-pareto-front-in-python\n",
    "def is_pareto_optimal(costs):\n",
    "    is_efficient = np.all(np.logical_not(np.isnan(costs)), axis=1)\n",
    "    for i, c in enumerate(costs):\n",
    "        if is_efficient[i]:\n",
    "            is_efficient[is_efficient] = np.any(costs[is_efficient]<c, axis=1)  # Keep any point with a lower cost\n",
    "            is_efficient[i] = True  # And keep self\n",
    "    return is_efficient\n",
    "    \n",
    "# function that receives a groupby subset and only returns the pareto front\n",
    "def apply_pareto(group, cost1, cost2, cost1_max = False, cost2_max = False):\n",
    "    # get the costs\n",
    "    costs = group[[cost1, cost2]].values.astype(float)\n",
    "    # invert the costs if they are maximization problems\n",
    "    if cost1_max:\n",
    "        costs[:,0] = -costs[:,0]\n",
    "    if cost2_max:\n",
    "        costs[:,1] = -costs[:,1]\n",
    "    # get the pareto front\n",
    "    return group[is_pareto_optimal(costs)]\n",
    "\n",
    "# function that receives a groupby subset and returns the complete pareto front by adding extreme points if not present already\n",
    "def complete_pareto_front(group):\n",
    "    full = group.copy()\n",
    "    if full['workflow.initial_coverage'].max() < 1000:\n",
    "        add = pd.DataFrame.from_dict({'codec.type': full['codec.type'].iloc[0], 'codec.name': full['codec.name'].iloc[0], 'workflow.type': full['workflow.type'].iloc[0], 'workflow.initial_coverage': 1000, 'workflow.sequencing_depth': 0.999*full['workflow.sequencing_depth'].min()}, orient='index').T\n",
    "        full = pd.concat([full, add], ignore_index=True).reset_index(drop=True)\n",
    "    if full['workflow.sequencing_depth'].max() < 1000:\n",
    "        add = pd.DataFrame.from_dict({'codec.type': full['codec.type'].iloc[0], 'codec.name': full['codec.name'].iloc[0], 'workflow.type': full['workflow.type'].iloc[0], 'workflow.initial_coverage': 0.999*full['workflow.initial_coverage'].min(), 'workflow.sequencing_depth': 1000}, orient='index').T\n",
    "        full = pd.concat([full, add], ignore_index=True).reset_index(drop=True)\n",
    "    return full.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the fits for both scenarios, and harmonize the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initcov = data_initcov.get_fits_by_group(['codec.type', 'codec.name', 'workflow.name', 'workflow.type', 'workflow.initial_coverage'], 'workflow.sequencing_depth', additional_agg={'code_rate': 'mean', 'n_sequences': 'mean', 'sequence_length': 'mean', 'n_bases': 'mean', 'filesize_bit': 'mean'})\n",
    "df_initcov['workflow.sequencing_depth'] = df_initcov['threshold']\n",
    "\n",
    "df_seqdepth = data_seqdepth.get_fits_by_group(['codec.type', 'codec.name', 'workflow.name', 'workflow.type', 'workflow.sequencing_depth'], 'workflow.initial_coverage', additional_agg={'code_rate': 'mean', 'n_sequences': 'mean', 'sequence_length': 'mean', 'n_bases': 'mean', 'filesize_bit': 'mean'})\n",
    "df_seqdepth['workflow.initial_coverage'] = df_seqdepth['threshold']\n",
    "\n",
    "df = pd.concat([df_initcov, df_seqdepth], ignore_index=True)\n",
    "df['workflow.sequencing_depth'] = df['workflow.sequencing_depth'].astype(float)\n",
    "df['workflow.initial_coverage'] = df['workflow.initial_coverage'].astype(float)\n",
    "\n",
    "df['eff_code_rate'] = df['code_rate'].astype(float) / df['workflow.initial_coverage'].astype(float)\n",
    "df['eff_storage_density'] = 122.2 * df['code_rate'].astype(float) / df['workflow.initial_coverage'].astype(float)\n",
    "df['name'] = df['codec.type'] + '-' + df['codec.name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply the pareto filter to remove non-optimal points for initial coverage + sequencing depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = df.groupby(['codec.type', 'codec.name', 'workflow.type']).apply(apply_pareto, 'workflow.initial_coverage', 'workflow.sequencing_depth', cost1_max = False, cost2_max = False, include_groups=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotdf = idf.groupby(['codec.type', 'codec.name', 'workflow.type'])[['codec.type', 'codec.name', 'workflow.type', 'workflow.initial_coverage', 'workflow.sequencing_depth']].apply(complete_pareto_front).reset_index(drop=True)\n",
    "\n",
    "plotdf = plotdf.sort_values(['codec.type', 'workflow.sequencing_depth', 'workflow.initial_coverage'])\n",
    "plotdf['codec.name'] = plotdf['codec.name'].replace({'high': 'High', 'medium': 'Medium', 'low': 'Low'})\n",
    "plotdf.loc[plotdf['codec.type'] == 'YinYang', 'codec.name'] = 'High'\n",
    "plotdf.loc[plotdf['codec.type'] == 'Goldman', 'codec.name'] = 'Low'\n",
    "plotdf['name'] = plotdf['codec.type'] + '-' + plotdf['codec.name']\n",
    "plotdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(\n",
    "    plotdf,\n",
    "    x='workflow.initial_coverage', \n",
    "    y='workflow.sequencing_depth', \n",
    "    log_y=True, \n",
    "    log_x=True, \n",
    "    color='codec.type', \n",
    "    facet_row='workflow.type', \n",
    "    facet_row_spacing=0.09,\n",
    "    facet_col='codec.name',\n",
    "    facet_col_spacing=0.06,\n",
    "    markers=True,\n",
    "    color_discrete_map=colormap_type,\n",
    "    category_orders={'codec.name': ['High', 'Medium', 'Low'], 'workflow.type': ['BestCase', 'WorstCase']},\n",
    "    range_x=[0.3, 500],\n",
    "    range_y=[0.3, 500],\n",
    ")\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    width=320,\n",
    "    height=240,\n",
    "    margin=dict(l=0, r=10, t=20, b=0),\n",
    ")\n",
    "fig.add_hline(\n",
    "    y=30,\n",
    "    line_dash='dash',\n",
    "    line_width=1,\n",
    ")\n",
    "fig.update_xaxes(dtick=1)\n",
    "fig.update_yaxes(dtick=1)\n",
    "fig.update_xaxes(title='Initial coverage', row=1)\n",
    "fig.update_yaxes(title='Sequencing depth', col=1)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "fig = dt4dds_benchmark.analysis.plotting.standardize_plot(fig)\n",
    "fig.write_image('./figures/pareto_front_cov_by_rate.svg')\n",
    "fig.write_image('./figures/pareto_front_cov_by_rate.png', scale=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the storage density at a sequencing depth of 30x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each workflow.type, codec.type and codec.name, interpolate initial coverage vs. sequencing depth\n",
    "storage_densities = []\n",
    "for workflow_type in idf['workflow.type'].unique():\n",
    "    for codec_type in idf['codec.type'].unique():\n",
    "        for codec_name in idf['codec.name'].unique():\n",
    "            subset = idf[(idf['workflow.type'] == workflow_type) & (idf['codec.type'] == codec_type) & (idf['codec.name'] == codec_name)]\n",
    "            subset = subset.sort_values('workflow.sequencing_depth', ascending=True)\n",
    "            if subset.shape[0] > 1:\n",
    "                y = subset['workflow.initial_coverage'].astype(float)\n",
    "                x = subset['workflow.sequencing_depth'].astype(float)\n",
    "                red = np.interp([30.0], x, y)[0]\n",
    "                storage_densities.append({\n",
    "                    'workflow.type': workflow_type,\n",
    "                    'codec.type': codec_type,\n",
    "                    'codec.name': codec_name,\n",
    "                    'code_rate': subset['code_rate'].mean(),\n",
    "                    'physical_redundancy': red,\n",
    "                    'storage_density': 113.7*subset['code_rate'].mean()/red,\n",
    "                })\n",
    "\n",
    "storage_densities = pd.DataFrame(storage_densities)\n",
    "storage_densities['code_rate'] = storage_densities['code_rate'].map('{:,.2f}'.format)\n",
    "storage_densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = storage_densities.sort_values(['codec.type', 'code_rate'])\n",
    "plot_df['codec.type'] = plot_df['codec.type'].replace({'YinYang': 'YY', 'Goldman': 'GM'})\n",
    "\n",
    "fig = dt4dds_benchmark.analysis.plotting.tiered_bar(\n",
    "    plot_df,\n",
    "    \"codec.type\",\n",
    "    \"code_rate\",\n",
    "    \"storage_density\",\n",
    "    color_by=\"workflow.type\",\n",
    "    color_discrete_map={'BestCase': '#3182bd', 'WorstCase': '#de2d26'},\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=350,\n",
    "    height=120,\n",
    "    margin=dict(l=0, r=2, t=10, b=30),\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.update_yaxes(title='Storage density / EB g<sup>-1</sup>', range=[0, 130])\n",
    "fig = dt4dds_benchmark.analysis.plotting.standardize_plot(fig)\n",
    "fig.update_xaxes(\n",
    "    tickfont_size=28/3, \n",
    "    tickangle=0,\n",
    ")\n",
    "fig.show()\n",
    "fig.write_image(f'./figures/comp.svg')\n",
    "fig.write_image(f'./figures/comp.png', scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performancedf_dropout = pd.merge(data_initcov.combined_performances, data_initcov.results)\n",
    "performancedf_dropout = performancedf_dropout.drop(performancedf_dropout.loc[performancedf_dropout['decoding_success'] == False].index)\n",
    "performancedf_dropout = performancedf_dropout.drop(performancedf_dropout.loc[performancedf_dropout['identifier'] != b'decoding'].index)\n",
    "\n",
    "performancedf_rate = pd.merge(data_seqdepth.combined_performances, data_seqdepth.results)\n",
    "performancedf_rate = performancedf_rate.drop(performancedf_rate.loc[performancedf_rate['decoding_success'] == False].index)\n",
    "performancedf_rate = performancedf_rate.drop(performancedf_rate.loc[performancedf_rate['identifier'] != b'decoding'].index)\n",
    "\n",
    "performancedf = pd.concat([performancedf_dropout, performancedf_rate], ignore_index=True)\n",
    "performancedf['codec.name'] = performancedf['codec.name'].map({\n",
    "    'default': '1low',\n",
    "    'low': '1low',\n",
    "    'medium': '2medium',\n",
    "    'high': '3high',\n",
    "})\n",
    "performancedf.loc[performancedf['codec.type'] == 'YinYang', 'codec.name'] = '3high'\n",
    "performancedf.sort_values(['codec.type', 'codec.name', 'duration'], inplace=True)\n",
    "performancedf['duration'] = performancedf['duration']/60\n",
    "\n",
    "performancedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    performancedf.loc[performancedf['workflow.type'] == 'BestCase'],\n",
    "    x='workflow.initial_coverage',\n",
    "    y='workflow.sequencing_depth',\n",
    "    log_x=True,\n",
    "    log_y=True,\n",
    "    color='duration',\n",
    "    facet_col='codec.name',\n",
    "    facet_col_spacing=0.05,\n",
    "    facet_row='codec.type',\n",
    "    facet_row_spacing=0.03,\n",
    "    range_color=(0, 60),\n",
    "    color_continuous_scale='Inferno',\n",
    ")\n",
    "\n",
    "fig.for_each_xaxis(lambda xaxis: xaxis.update(range=[-0.5, 3.1], tickangle=0))\n",
    "fig.for_each_yaxis(lambda yaxis: yaxis.update(range=[-0.5, 3.1]))\n",
    "fig.update_xaxes(title='Physical coverage', row=1)\n",
    "fig.update_xaxes(showticklabels=True, row=2)\n",
    "fig.update_yaxes(title='Sequencing depth', col=1)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.update_layout(\n",
    "    width=680,\n",
    "    height=800,\n",
    "    margin=dict(l=0, r=10, t=1, b=10),\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.update_coloraxes(colorbar={'orientation':'h', 'thickness': 20, 'y': 10.0})\n",
    "fig = dt4dds_benchmark.analysis.plotting.standardize_plot(fig)\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(f'./figures/runtime_best.svg')\n",
    "fig.write_image(f'./figures/runtime_best.png', scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    performancedf.loc[performancedf['workflow.type'] == 'WorstCase'],\n",
    "    x='workflow.initial_coverage',\n",
    "    y='workflow.sequencing_depth',\n",
    "    log_x=True,\n",
    "    log_y=True,\n",
    "    color='duration',\n",
    "    facet_col='codec.name',\n",
    "    facet_col_spacing=0.05,\n",
    "    facet_row='codec.type',\n",
    "    facet_row_spacing=0.08,\n",
    "    range_color=(0, 60),\n",
    "    color_continuous_scale='Inferno',\n",
    ")\n",
    "\n",
    "fig.for_each_xaxis(lambda xaxis: xaxis.update(range=[0, 3.1], tickangle=0))\n",
    "fig.for_each_yaxis(lambda yaxis: yaxis.update(range=[0, 3.1]))\n",
    "fig.update_xaxes(title='Physical coverage', row=1)\n",
    "# fig.update_xaxes(showticklabels=True, row=2)\n",
    "fig.update_yaxes(title='Sequencing depth', col=1)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.update_layout(\n",
    "    width=680,\n",
    "    height=500,\n",
    "    margin=dict(l=0, r=10, t=1, b=10),\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.update_coloraxes(colorbar={'orientation':'h', 'thickness': 20, 'y': 10.0})\n",
    "fig = dt4dds_benchmark.analysis.plotting.standardize_plot(fig)\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(f'./figures/runtime_worst.svg')\n",
    "fig.write_image(f'./figures/runtime_worst.png', scale=2)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
